{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P99wDod7uhnz",
        "outputId": "8109c707-fdd7-44fa-e889-d6da54a144c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.2)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.41)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.39.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "GPU name:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "from wandb.keras import WandbMetricsLogger\n",
        "\n",
        "import keras\n",
        "from keras.callbacks import Callback\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
        "from keras.layers import Input, Conv2D, Conv2DTranspose, Add, Activation\n",
        "from keras.models import Model\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "print('GPU name: ', tf.config.experimental.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    cwd = Path().cwd()\n",
        "\n",
        "    moritz = \"/content/drive/My Drive/Colab/CV23_Data/preprocessed_data/Datasets.zip\"\n",
        "    david = \"/content/drive/My Drive/CV23_Data/preprocessed_data/Datasets.zip\"\n",
        "    max = cwd / 'drive' / 'My Drive' / 'cvue23' / 'Datasets.zip'\n",
        "    waad = cwd / 'drive' / 'My Drive' / 'CV24' / 'Datasets.zip'\n",
        "\n",
        "\n",
        "    zip_ref = zipfile.ZipFile(max, 'r')\n",
        "    zip_ref.extractall(\"/tmp\")\n",
        "    zip_ref.close()\n",
        "\n",
        "    checkpoint_path = cwd / 'drive' / 'My Drive' / 'cvue23' / 'model_checkpoints'\n",
        "    hyperparameter_path = cwd / 'drive' / 'My Drive' / 'cvue23' / 'hyperparameter_logs'\n",
        "    checkpoint_path.mkdir(exist_ok=True)\n",
        "    datasets_path = cwd.parent.parent / 'tmp'\n",
        "\n",
        "except:\n",
        "    cwd = Path().cwd().parent\n",
        "    checkpoint_path = cwd / 'model_checkpoints'\n",
        "    hyperparameter_path = cwd / 'hyperparameter_logs'\n",
        "    checkpoint_path.mkdir(exist_ok=True)\n",
        "    datasets_path = cwd / 'data'"
      ],
      "metadata": {
        "id": "-7j46u7a5CdY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94f635f4-5d53-44b1-cb96-0f028942f5df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.4.6)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2023.11.17)\n",
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_path = datasets_path / 'training_set'\n",
        "val_set_original_path = datasets_path / 'validation_set_original'\n",
        "val_set_crop_path = datasets_path / 'validation_set_cropped'\n",
        "\n",
        "assert (train_set_path.exists() and val_set_original_path.exists() and val_set_crop_path.exists())"
      ],
      "metadata": {
        "id": "ecgxdd3H5i_s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-4wKc59hTLxF"
      },
      "outputs": [],
      "source": [
        "map_label_to_name = ['no_person', 'idle','sitting', 'laying']\n",
        "\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    def __init__(\n",
        "        self,\n",
        "        basedir: Path,\n",
        "        batch_size: int = None,\n",
        "        included_poses: list = None,\n",
        "        included_trees: list = None,\n",
        "        shuffle=False,\n",
        "        only_use_n: int = None,\n",
        "        random_rotation: bool = False,\n",
        "        random_flip: bool = False,\n",
        "    ):\n",
        "        if not basedir.exists():\n",
        "            ValueError('Datafolder does not exist. Add it to your drive and try again. Maybe restart the runtime.')\n",
        "\n",
        "        self.basedir = basedir\n",
        "        self.batch_size = batch_size\n",
        "        self.included_poses = [map_label_to_name.index(pose) for pose in included_poses] if included_poses is not None else None\n",
        "        self.included_trees  = included_trees\n",
        "        self.filenames = self.__filter(shuffle, only_use_n)\n",
        "        self.random_flip = random_flip\n",
        "        self.random_rotation = random_rotation\n",
        "\n",
        "    def __filter(self, shuffle, only_use_n):\n",
        "\n",
        "        files = []\n",
        "        self.pose_distribution = defaultdict(int)\n",
        "        self.trees_distribution = defaultdict(int)\n",
        "        self.pose_distribution_filtered = defaultdict(int)\n",
        "        self.trees_distribution_filtered = defaultdict(int)\n",
        "\n",
        "        unfiltered = list(self.basedir.iterdir())\n",
        "\n",
        "        if shuffle:\n",
        "            random.shuffle(unfiltered)\n",
        "\n",
        "        total = len(unfiltered)\n",
        "        if only_use_n is not None:\n",
        "            total = only_use_n\n",
        "\n",
        "        for path in tqdm(unfiltered, total=total):\n",
        "\n",
        "            loaded = np.load(path)\n",
        "            pose, trees = loaded['pose'], loaded['trees']\n",
        "\n",
        "            self.pose_distribution[pose.item()] += 1\n",
        "            self.trees_distribution[trees.item()] += 1\n",
        "\n",
        "            fname = path.name\n",
        "            if self.included_poses is not None and pose not in self.included_poses:\n",
        "                continue\n",
        "\n",
        "            if self.included_trees is not None and trees not in self.included_trees:\n",
        "                continue\n",
        "\n",
        "            files.append(fname)\n",
        "            self.pose_distribution_filtered[pose.item()] += 1\n",
        "            self.trees_distribution_filtered[trees.item()] += 1\n",
        "\n",
        "            if only_use_n is not None and len(files) == only_use_n:\n",
        "                break\n",
        "\n",
        "        return files\n",
        "\n",
        "    def load(self, path):\n",
        "        loaded = np.load(path)\n",
        "        x = loaded['x'] / 255\n",
        "        y = loaded['y'] / 255\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.batch_size is None:\n",
        "            return len(self.filenames)\n",
        "\n",
        "        return math.ceil(len(self.filenames) / self.batch_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if self.batch_size is None:\n",
        "            batch = self.filenames\n",
        "        else:\n",
        "            low = idx * self.batch_size\n",
        "            high = min(low + self.batch_size, len(self.filenames))\n",
        "            batch = self.filenames[low:high]\n",
        "\n",
        "        X, Y = [],[]\n",
        "        for fname in batch:\n",
        "            x,y = self.load(self.basedir / fname)\n",
        "\n",
        "            flip = self.random_flip and bool(random.getrandbits(1))\n",
        "\n",
        "            x = np.fliplr(x) if flip else x\n",
        "            y = np.fliplr(y) if flip else y\n",
        "            X.append(x)\n",
        "            Y.append(y)\n",
        "\n",
        "        return np.stack(X), np.stack(Y)\n",
        "\n",
        "    def print_info(self):\n",
        "        print()\n",
        "        shape = self.load(self.basedir / self.filenames[0])[0].shape\n",
        "        print(f'{len(self.filenames)} samples with shape : {shape}')\n",
        "\n",
        "        print(f'Pose distribution total')\n",
        "        (\"{:<15} {:<15}\".format('pose', 'number of samples'))\n",
        "        for key, value in self.pose_distribution.items():\n",
        "            print(\"{:<15} {:<15}\".format(map_label_to_name[key], value))\n",
        "        print()\n",
        "        print(f'Pose distribution filtered')\n",
        "        (\"{:<15} {:<15}\".format('pose', 'number of samples'))\n",
        "        for key, value in self.pose_distribution_filtered.items():\n",
        "            print(\"{:<15} {:<15}\".format(map_label_to_name[key], value))\n",
        "\n",
        "        print()\n",
        "        print(f'Trees distribution total')\n",
        "        print(\"{:<15} {:<15}\".format('num trees per ha', 'number of samples'))\n",
        "\n",
        "        for key, value in self.trees_distribution.items():\n",
        "            print(\"{:<15} {:<15}\".format(key, value))\n",
        "\n",
        "        print()\n",
        "        print(f'Trees distribution filtered')\n",
        "        print(\"{:<15} {:<15}\".format('num trees per ha', 'number of samples'))\n",
        "\n",
        "        for key, value in self.trees_distribution_filtered.items():\n",
        "            print(\"{:<15} {:<15}\".format(key, value))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Validation Dataset')\n",
        "validation_data = DataGenerator(\n",
        "    val_set_crop_path,\n",
        "    only_use_n=128,\n",
        "    shuffle=True,\n",
        ")\n",
        "validation_data.print_info()"
      ],
      "metadata": {
        "id": "mDMP_4FyA9Kb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "848ae48b-a065-4006-8367-c6cac2a6191d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 99%|█████████▉| 127/128 [00:00<00:00, 1770.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "128 samples with shape : (128, 128, 6)\n",
            "Pose distribution total\n",
            "sitting         37             \n",
            "laying          43             \n",
            "idle            31             \n",
            "no_person       17             \n",
            "\n",
            "Pose distribution filtered\n",
            "sitting         37             \n",
            "laying          43             \n",
            "idle            31             \n",
            "no_person       17             \n",
            "\n",
            "Trees distribution total\n",
            "num trees per ha number of samples\n",
            "0               13             \n",
            "100             76             \n",
            "200             39             \n",
            "\n",
            "Trees distribution filtered\n",
            "num trees per ha number of samples\n",
            "0               13             \n",
            "100             76             \n",
            "200             39             \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "X32XWUeqtdyY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82e118e7-533d-43e9-f598-6e1bae01f281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27281/27281 [00:15<00:00, 1757.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "24632 samples with shape : (128, 128, 6)\n",
            "Pose distribution total\n",
            "idle            8182           \n",
            "sitting         8237           \n",
            "no_person       2649           \n",
            "laying          8213           \n",
            "\n",
            "Pose distribution filtered\n",
            "idle            8182           \n",
            "sitting         8237           \n",
            "laying          8213           \n",
            "\n",
            "Trees distribution total\n",
            "num trees per ha number of samples\n",
            "0               2807           \n",
            "200             8133           \n",
            "100             16341          \n",
            "\n",
            "Trees distribution filtered\n",
            "num trees per ha number of samples\n",
            "0               2552           \n",
            "200             7371           \n",
            "100             14709          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print('Training Dataset')\n",
        "\n",
        "train_data = DataGenerator(\n",
        "    train_set_path,\n",
        "    batch_size=128,\n",
        "    included_poses=['idle','sitting', 'laying'],\n",
        "    shuffle=True,\n",
        "    random_flip=True,\n",
        ")\n",
        "train_data.print_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "p7MK0yc5tadN"
      },
      "outputs": [],
      "source": [
        "def encoder(x, num_features, num_layers, residual_every=2):\n",
        "    x = Conv2D(num_features, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Save the output of conv layers at even indices\n",
        "    residuals = []\n",
        "\n",
        "    # Encoder\n",
        "    for i in range(num_layers - 1):\n",
        "        x = Conv2D(num_features, kernel_size=3, padding='same', activation='relu')(x)\n",
        "        if (i + 1) % residual_every == 0:\n",
        "            residuals.append(x)\n",
        "\n",
        "    return x, residuals\n",
        "\n",
        "def decoder(x, num_features, num_layers, residuals, residual_every=2):\n",
        "\n",
        "    # Decoder\n",
        "    for i in range(num_layers - 1):\n",
        "        x = Conv2DTranspose(num_features, kernel_size=3, padding='same')(x)\n",
        "\n",
        "        if (i + 1 + num_layers) % residual_every == 0 and residuals:\n",
        "            res = residuals.pop()\n",
        "            x = Add()([x, res])\n",
        "\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    if residuals: raise ValueError('There are unused residual connections')\n",
        "\n",
        "    # create 1-channel output\n",
        "    x = Conv2DTranspose(1, kernel_size=3, strides=2, padding='same')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def REDNet(num_layers, num_features, channel_size):\n",
        "    '''Model definition with keras functional layers api'''\n",
        "\n",
        "    inputs = Input(shape=(None, None, channel_size))\n",
        "\n",
        "    x, residuals = encoder(inputs, num_features, num_layers)\n",
        "\n",
        "    x = decoder(x, num_features, num_layers, residuals)\n",
        "\n",
        "    # Add input residual, needed to do 1x1 conv to adapt channels\n",
        "    residual = Conv2DTranspose(1, kernel_size=1, padding='same')(inputs)\n",
        "    outputs = Add()([x, residual])\n",
        "    outputs = Activation('relu')(outputs)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs=inputs, outputs=outputs, name=f'REDNet{num_layers*2}')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "L-1D9y9dvuyA"
      },
      "outputs": [],
      "source": [
        "# compile the model\n",
        "model = REDNet(\n",
        "    num_layers=11,\n",
        "    num_features=64,\n",
        "    channel_size=6\n",
        ")\n",
        "\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=1000,\n",
        "    decay_rate=0.9,\n",
        "    staircase=True\n",
        ")\n",
        "opt = keras.optimizers.Adam(\n",
        "    learning_rate=lr_schedule\n",
        ")\n",
        "loss = keras.losses.MeanSquaredError( reduction=\"sum_over_batch_size\")\n",
        "\n",
        "model.compile(loss=loss,optimizer=opt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = Path.cwd() / 'checkpoints_from_scratch'\n",
        "\n",
        "#model.load_weights((Path.cwd() / 'checkpoints' / '04-0.01.keras'))"
      ],
      "metadata": {
        "id": "N3e524oASoJ8"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "WdV3uwPPv2AO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9952df75-ff96-4cff-e517-1ceeff76c2ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "193/193 [==============================] - 76s 360ms/step - loss: 0.0545 - val_loss: 0.0505\n",
            "Epoch 2/20\n",
            "193/193 [==============================] - 70s 362ms/step - loss: 0.0439 - val_loss: 0.0340\n",
            "Epoch 3/20\n",
            "193/193 [==============================] - 70s 363ms/step - loss: 0.0371 - val_loss: 0.0278\n",
            "Epoch 4/20\n",
            "193/193 [==============================] - 70s 364ms/step - loss: 0.0277 - val_loss: 0.0260\n",
            "Epoch 5/20\n",
            "193/193 [==============================] - 70s 364ms/step - loss: 0.0225 - val_loss: 0.0214\n",
            "Epoch 6/20\n",
            "193/193 [==============================] - 70s 363ms/step - loss: 0.0196 - val_loss: 0.0243\n",
            "Epoch 7/20\n",
            "193/193 [==============================] - 71s 365ms/step - loss: 0.0186 - val_loss: 0.0159\n",
            "Epoch 8/20\n",
            "193/193 [==============================] - 69s 357ms/step - loss: 0.0165 - val_loss: 0.0177\n",
            "Epoch 9/20\n",
            "193/193 [==============================] - 73s 379ms/step - loss: 0.0163 - val_loss: 0.0190\n",
            "Epoch 10/20\n",
            "193/193 [==============================] - 70s 361ms/step - loss: 0.0150 - val_loss: 0.0137\n",
            "Epoch 11/20\n",
            "193/193 [==============================] - 71s 366ms/step - loss: 0.0137 - val_loss: 0.0159\n",
            "Epoch 12/20\n",
            "193/193 [==============================] - 71s 369ms/step - loss: 0.0135 - val_loss: 0.0121\n",
            "Epoch 13/20\n",
            "193/193 [==============================] - 70s 361ms/step - loss: 0.0127 - val_loss: 0.0125\n",
            "Epoch 14/20\n",
            "193/193 [==============================] - 71s 367ms/step - loss: 0.0114 - val_loss: 0.0123\n",
            "Epoch 15/20\n",
            "193/193 [==============================] - 69s 360ms/step - loss: 0.0114 - val_loss: 0.0105\n",
            "Epoch 16/20\n",
            "193/193 [==============================] - 70s 363ms/step - loss: 0.0108 - val_loss: 0.0105\n",
            "Epoch 17/20\n",
            "193/193 [==============================] - 69s 360ms/step - loss: 0.0100 - val_loss: 0.0105\n",
            "Epoch 18/20\n",
            "193/193 [==============================] - 71s 366ms/step - loss: 0.0101 - val_loss: 0.0124\n",
            "Epoch 19/20\n",
            "193/193 [==============================] - 71s 368ms/step - loss: 0.0096 - val_loss: 0.0099\n",
            "Epoch 20/20\n",
            "193/193 [==============================] - 73s 379ms/step - loss: 0.0092 - val_loss: 0.0120\n"
          ]
        }
      ],
      "source": [
        "callbacks = [\n",
        "    ModelCheckpoint((checkpoint_dir / 'ep{epoch:02d}_loss{val_loss:.4f}.keras').as_posix(), save_best_only=True),\n",
        "    CSVLogger(checkpoint_dir / 'training_max.csv', append=True),\n",
        "    #keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0005),\n",
        "    #keras.callbacks.ProgbarLogger(count_mode='steps'),\n",
        "    #WandbMetricsLogger(),\n",
        "]\n",
        "# train on the dataset\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=validation_data,\n",
        "    epochs=20,\n",
        "    callbacks=callbacks,\n",
        "    shuffle=True,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "uDr5Cc3D9yGR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a822922a-6ad0-47ec-e678-07c38a454a85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "193/193 [==============================] - 72s 373ms/step - loss: 0.0089 - val_loss: 0.0097\n",
            "Epoch 2/10\n",
            "193/193 [==============================] - 69s 360ms/step - loss: 0.0085 - val_loss: 0.0116\n",
            "Epoch 3/10\n",
            "193/193 [==============================] - 71s 368ms/step - loss: 0.0089 - val_loss: 0.0087\n",
            "Epoch 4/10\n",
            "193/193 [==============================] - 73s 377ms/step - loss: 0.0084 - val_loss: 0.0107\n",
            "Epoch 5/10\n",
            "193/193 [==============================] - 75s 387ms/step - loss: 0.0085 - val_loss: 0.0083\n",
            "Epoch 6/10\n",
            "193/193 [==============================] - 69s 360ms/step - loss: 0.0077 - val_loss: 0.0085\n",
            "Epoch 7/10\n",
            "193/193 [==============================] - 70s 365ms/step - loss: 0.0074 - val_loss: 0.0079\n",
            "Epoch 8/10\n",
            "193/193 [==============================] - 69s 359ms/step - loss: 0.0073 - val_loss: 0.0106\n",
            "Epoch 9/10\n",
            "193/193 [==============================] - 74s 381ms/step - loss: 0.0076 - val_loss: 0.0077\n",
            "Epoch 10/10\n",
            "193/193 [==============================] - 72s 373ms/step - loss: 0.0075 - val_loss: 0.0071\n"
          ]
        }
      ],
      "source": [
        "# train on the dataset\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=validation_data,\n",
        "    epochs=10,\n",
        "    callbacks=callbacks,\n",
        "    shuffle=True,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.optimizer.learning_rate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48gRZoDvp3DD",
        "outputId": "ec91fc11-767d-41ee-a969-df8954d1dc59"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'current_learning_rate:0' shape=() dtype=float32, numpy=0.00059048994>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train on the dataset\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=validation_data,\n",
        "    epochs=40,\n",
        "    callbacks=callbacks,\n",
        "    shuffle=True,\n",
        "    verbose=1,\n",
        "    initial_epoch=31,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOqM8cFznJR9",
        "outputId": "fd23264b-e594-4424-af52-8f50840f0987"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/40\n",
            "193/193 [==============================] - 78s 371ms/step - loss: 0.0054 - val_loss: 0.0067\n",
            "Epoch 33/40\n",
            "193/193 [==============================] - 69s 359ms/step - loss: 0.0053 - val_loss: 0.0063\n",
            "Epoch 34/40\n",
            "193/193 [==============================] - 69s 358ms/step - loss: 0.0053 - val_loss: 0.0059\n",
            "Epoch 35/40\n",
            "193/193 [==============================] - 70s 364ms/step - loss: 0.0053 - val_loss: 0.0060\n",
            "Epoch 36/40\n",
            "193/193 [==============================] - 72s 371ms/step - loss: 0.0052 - val_loss: 0.0063\n",
            "Epoch 37/40\n",
            "193/193 [==============================] - 70s 360ms/step - loss: 0.0051 - val_loss: 0.0057\n",
            "Epoch 38/40\n",
            "193/193 [==============================] - 68s 353ms/step - loss: 0.0051 - val_loss: 0.0057\n",
            "Epoch 39/40\n",
            "193/193 [==============================] - 68s 352ms/step - loss: 0.0050 - val_loss: 0.0062\n",
            "Epoch 40/40\n",
            "193/193 [==============================] - 73s 378ms/step - loss: 0.0050 - val_loss: 0.0055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.optimizer.learning_rate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCoFj70aqVYt",
        "outputId": "b1a039a1-75a4-442e-b57f-a3896745b724"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'current_learning_rate:0' shape=() dtype=float32, numpy=0.0004782968>"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt = keras.optimizers.Adam(\n",
        "    learning_rate=0.0001\n",
        ")\n",
        "loss = keras.losses.MeanSquaredError( reduction=\"sum_over_batch_size\")\n",
        "\n",
        "model.compile(loss=loss,optimizer=opt)"
      ],
      "metadata": {
        "id": "tfkhDvqQs7da"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=validation_data,\n",
        "    epochs=60,\n",
        "    callbacks=callbacks,\n",
        "    shuffle=True,\n",
        "    verbose=1,\n",
        "    initial_epoch=49,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhyaEtSutjrz",
        "outputId": "1ff6b003-84cc-40fa-904d-2a411070f8c7"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/60\n",
            "193/193 [==============================] - 73s 376ms/step - loss: 0.0050 - val_loss: 0.0057\n",
            "Epoch 51/60\n",
            "193/193 [==============================] - 70s 363ms/step - loss: 0.0050 - val_loss: 0.0055\n",
            "Epoch 52/60\n",
            "193/193 [==============================] - 70s 364ms/step - loss: 0.0049 - val_loss: 0.0055\n",
            "Epoch 53/60\n",
            "193/193 [==============================] - 69s 358ms/step - loss: 0.0049 - val_loss: 0.0057\n",
            "Epoch 54/60\n",
            "193/193 [==============================] - 72s 372ms/step - loss: 0.0048 - val_loss: 0.0058\n",
            "Epoch 55/60\n",
            "193/193 [==============================] - 70s 362ms/step - loss: 0.0049 - val_loss: 0.0053\n",
            "Epoch 56/60\n",
            "193/193 [==============================] - 69s 355ms/step - loss: 0.0048 - val_loss: 0.0055\n",
            "Epoch 57/60\n",
            "193/193 [==============================] - 72s 371ms/step - loss: 0.0048 - val_loss: 0.0053\n",
            "Epoch 58/60\n",
            "193/193 [==============================] - 73s 378ms/step - loss: 0.0047 - val_loss: 0.0055\n",
            "Epoch 59/60\n",
            "193/193 [==============================] - 69s 359ms/step - loss: 0.0047 - val_loss: 0.0058\n",
            "Epoch 60/60\n",
            "193/193 [==============================] - 69s 360ms/step - loss: 0.0047 - val_loss: 0.0051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now train on the full size validation, but with a split"
      ],
      "metadata": {
        "id": "NRfBOg4NvgZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_train_data = DataGenerator(\n",
        "    val_set_original_path,\n",
        "    shuffle=False,\n",
        "    random_flip=True,\n",
        "    batch_size=32,\n",
        "    only_use_n=4815-128\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIwe0vn-u1eu",
        "outputId": "3c2f1373-097e-4545-cf15-19f66b1cda6f"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 4686/4687 [00:02<00:00, 1816.27it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valset = DataGenerator(\n",
        "    val_set_crop_path,\n",
        "    shuffle=False,\n",
        "    random_flip=True,\n",
        ")\n",
        "valset.filenames = valset.filenames[-128:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmkpc1UF1YKa",
        "outputId": "1bca492b-0601-4cb5-dcc3-9cc3863464c1"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4815/4815 [00:02<00:00, 1791.33it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    full_train_data,\n",
        "    validation_data=valset,\n",
        "    epochs=70,\n",
        "    callbacks=callbacks,\n",
        "    shuffle=True,\n",
        "    verbose=1,\n",
        "    initial_epoch=60,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVlLlTxw0nW_",
        "outputId": "3e10c046-e727-45e8-8326-d263a40252d1"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 61/70\n",
            "147/147 [==============================] - 259s 2s/step - loss: 0.0037 - val_loss: 0.0057\n",
            "Epoch 62/70\n",
            "147/147 [==============================] - 222s 2s/step - loss: 0.0035 - val_loss: 0.0065\n",
            "Epoch 63/70\n",
            "147/147 [==============================] - 239s 2s/step - loss: 0.0034 - val_loss: 0.0056\n",
            "Epoch 64/70\n",
            "147/147 [==============================] - 241s 2s/step - loss: 0.0033 - val_loss: 0.0054\n",
            "Epoch 65/70\n",
            "147/147 [==============================] - 194s 1s/step - loss: 0.0032 - val_loss: 0.0057\n",
            "Epoch 66/70\n",
            "147/147 [==============================] - 215s 1s/step - loss: 0.0033 - val_loss: 0.0073\n",
            "Epoch 67/70\n",
            "147/147 [==============================] - 224s 2s/step - loss: 0.0032 - val_loss: 0.0056\n",
            "Epoch 68/70\n",
            "147/147 [==============================] - 227s 2s/step - loss: 0.0033 - val_loss: 0.0074\n",
            "Epoch 69/70\n",
            "147/147 [==============================] - 227s 2s/step - loss: 0.0032 - val_loss: 0.0060\n",
            "Epoch 70/70\n",
            "147/147 [==============================] - 263s 2s/step - loss: 0.0032 - val_loss: 0.0058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(checkpoint_dir/'with_retrain.model.keras')"
      ],
      "metadata": {
        "id": "feoJNOl41UJ3"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TiHseHJYAmvB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}