{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P99wDod7uhnz",
        "outputId": "8109c707-fdd7-44fa-e889-d6da54a144c6"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbMetricsLogger\n",
        "\n",
        "import keras\n",
        "from keras.callbacks import Callback\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "from keras.layers import Input, Conv2D, Conv2DTranspose, Add, Activation\n",
        "from keras.models import Model\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import math\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import zipfile\n",
        "import random\n",
        "\n",
        "print('GPU name: ', tf.config.experimental.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7j46u7a5CdY",
        "outputId": "94f635f4-5d53-44b1-cb96-0f028942f5df"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    cwd = Path().cwd()\n",
        "\n",
        "    moritz = \"/content/drive/My Drive/Colab/CV23_Data/preprocessed_data/Datasets.zip\"\n",
        "    david = \"/content/drive/My Drive/CV23_Data/preprocessed_data/Datasets.zip\"\n",
        "    max = cwd / 'drive' / 'My Drive' / 'cvue23' / 'Datasets.zip'\n",
        "    waad = cwd / 'drive' / 'My Drive' / 'CV24' / 'Datasets.zip'\n",
        "\n",
        "\n",
        "    zip_ref = zipfile.ZipFile(max, 'r')\n",
        "    zip_ref.extractall(\"/tmp\")\n",
        "    zip_ref.close()\n",
        "\n",
        "    checkpoint_path = cwd / 'drive' / 'My Drive' / 'cvue23' / 'model_checkpoints'\n",
        "    hyperparameter_path = cwd / 'drive' / 'My Drive' / 'cvue23' / 'hyperparameter_logs'\n",
        "    checkpoint_path.mkdir(exist_ok=True)\n",
        "    datasets_path = cwd.parent.parent / 'tmp'\n",
        "\n",
        "except:\n",
        "    cwd = Path().cwd().parent\n",
        "    checkpoint_path = cwd / 'model_checkpoints'\n",
        "    hyperparameter_path = cwd / 'hyperparameter_logs'\n",
        "    checkpoint_path.mkdir(exist_ok=True)\n",
        "    datasets_path = cwd / 'data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecgxdd3H5i_s"
      },
      "outputs": [],
      "source": [
        "train_set_path = datasets_path / 'training_set'\n",
        "val_set_original_path = datasets_path / 'validation_set_original'\n",
        "val_set_crop_path = datasets_path / 'validation_set_cropped'\n",
        "\n",
        "assert (train_set_path.exists() and val_set_original_path.exists() and val_set_crop_path.exists())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4wKc59hTLxF"
      },
      "outputs": [],
      "source": [
        "map_label_to_name = ['no_person', 'idle','sitting', 'laying']\n",
        "\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    def __init__(\n",
        "        self,\n",
        "        basedir: Path,\n",
        "        batch_size: int = None,\n",
        "        included_poses: list = None,\n",
        "        included_trees: list = None,\n",
        "        shuffle=False,\n",
        "        only_use_n: int = None,\n",
        "        random_rotation: bool = False,\n",
        "        random_flip: bool = False,\n",
        "    ):\n",
        "        if not basedir.exists():\n",
        "            ValueError('Datafolder does not exist. Add it to your drive and try again. Maybe restart the runtime.')\n",
        "\n",
        "        self.basedir = basedir\n",
        "        self.batch_size = batch_size\n",
        "        self.included_poses = [map_label_to_name.index(pose) for pose in included_poses] if included_poses is not None else None\n",
        "        self.included_trees  = included_trees\n",
        "        self.filenames = self.__filter(shuffle, only_use_n)\n",
        "        self.random_flip = random_flip\n",
        "        self.random_rotation = random_rotation\n",
        "\n",
        "    def __filter(self, shuffle, only_use_n):\n",
        "\n",
        "        files = []\n",
        "        self.pose_distribution = defaultdict(int)\n",
        "        self.trees_distribution = defaultdict(int)\n",
        "        self.pose_distribution_filtered = defaultdict(int)\n",
        "        self.trees_distribution_filtered = defaultdict(int)\n",
        "\n",
        "        unfiltered = list(self.basedir.iterdir())\n",
        "\n",
        "        if shuffle:\n",
        "            random.shuffle(unfiltered)\n",
        "\n",
        "        total = len(unfiltered)\n",
        "        if only_use_n is not None:\n",
        "            total = only_use_n\n",
        "\n",
        "        for path in tqdm(unfiltered, total=total):\n",
        "\n",
        "            loaded = np.load(path)\n",
        "            pose, trees = loaded['pose'], loaded['trees']\n",
        "\n",
        "            self.pose_distribution[pose.item()] += 1\n",
        "            self.trees_distribution[trees.item()] += 1\n",
        "\n",
        "            fname = path.name\n",
        "            if self.included_poses is not None and pose not in self.included_poses:\n",
        "                continue\n",
        "\n",
        "            if self.included_trees is not None and trees not in self.included_trees:\n",
        "                continue\n",
        "\n",
        "            files.append(fname)\n",
        "            self.pose_distribution_filtered[pose.item()] += 1\n",
        "            self.trees_distribution_filtered[trees.item()] += 1\n",
        "\n",
        "            if only_use_n is not None and len(files) == only_use_n:\n",
        "                break\n",
        "\n",
        "        return files\n",
        "\n",
        "    def load(self, path):\n",
        "        loaded = np.load(path)\n",
        "        x = loaded['x'] / 255\n",
        "        y = loaded['y'] / 255\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.batch_size is None:\n",
        "            return len(self.filenames)\n",
        "\n",
        "        return math.ceil(len(self.filenames) / self.batch_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if self.batch_size is None:\n",
        "            batch = self.filenames\n",
        "        else:\n",
        "            low = idx * self.batch_size\n",
        "            high = min(low + self.batch_size, len(self.filenames))\n",
        "            batch = self.filenames[low:high]\n",
        "\n",
        "        X, Y = [],[]\n",
        "        for fname in batch:\n",
        "            x,y = self.load(self.basedir / fname)\n",
        "\n",
        "            flip = self.random_flip and bool(random.getrandbits(1))\n",
        "\n",
        "            x = np.fliplr(x) if flip else x\n",
        "            y = np.fliplr(y) if flip else y\n",
        "            X.append(x)\n",
        "            Y.append(y)\n",
        "\n",
        "        return np.stack(X), np.stack(Y)\n",
        "\n",
        "    def print_info(self):\n",
        "        print()\n",
        "        shape = self.load(self.basedir / self.filenames[0])[0].shape\n",
        "        print(f'{len(self.filenames)} samples with shape : {shape}')\n",
        "\n",
        "        print(f'Pose distribution total')\n",
        "        (\"{:<15} {:<15}\".format('pose', 'number of samples'))\n",
        "        for key, value in self.pose_distribution.items():\n",
        "            print(\"{:<15} {:<15}\".format(map_label_to_name[key], value))\n",
        "        print()\n",
        "        print(f'Pose distribution filtered')\n",
        "        (\"{:<15} {:<15}\".format('pose', 'number of samples'))\n",
        "        for key, value in self.pose_distribution_filtered.items():\n",
        "            print(\"{:<15} {:<15}\".format(map_label_to_name[key], value))\n",
        "\n",
        "        print()\n",
        "        print(f'Trees distribution total')\n",
        "        print(\"{:<15} {:<15}\".format('num trees per ha', 'number of samples'))\n",
        "\n",
        "        for key, value in self.trees_distribution.items():\n",
        "            print(\"{:<15} {:<15}\".format(key, value))\n",
        "\n",
        "        print()\n",
        "        print(f'Trees distribution filtered')\n",
        "        print(\"{:<15} {:<15}\".format('num trees per ha', 'number of samples'))\n",
        "\n",
        "        for key, value in self.trees_distribution_filtered.items():\n",
        "            print(\"{:<15} {:<15}\".format(key, value))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDMP_4FyA9Kb",
        "outputId": "848ae48b-a065-4006-8367-c6cac2a6191d"
      },
      "outputs": [],
      "source": [
        "print('Validation Dataset')\n",
        "validation_data = DataGenerator(\n",
        "    val_set_crop_path,\n",
        "    only_use_n=128,\n",
        "    shuffle=True,\n",
        ")\n",
        "validation_data.print_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X32XWUeqtdyY",
        "outputId": "82e118e7-533d-43e9-f598-6e1bae01f281"
      },
      "outputs": [],
      "source": [
        "print('Training Dataset')\n",
        "\n",
        "train_data = DataGenerator(\n",
        "    train_set_path,\n",
        "    batch_size=128,\n",
        "    included_poses=['idle','sitting', 'laying'],\n",
        "    shuffle=True,\n",
        "    random_flip=True,\n",
        ")\n",
        "train_data.print_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7MK0yc5tadN"
      },
      "outputs": [],
      "source": [
        "def encoder(x, num_features, num_layers, residual_every=2):\n",
        "    x = Conv2D(num_features, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Save the output of conv layers at even indices\n",
        "    residuals = []\n",
        "\n",
        "    # Encoder\n",
        "    for i in range(num_layers - 1):\n",
        "        x = Conv2D(num_features, kernel_size=3, padding='same', activation='relu')(x)\n",
        "        if (i + 1) % residual_every == 0:\n",
        "            residuals.append(x)\n",
        "\n",
        "    return x, residuals\n",
        "\n",
        "def decoder(x, num_features, num_layers, residuals, residual_every=2):\n",
        "\n",
        "    # Decoder\n",
        "    for i in range(num_layers - 1):\n",
        "        x = Conv2DTranspose(num_features, kernel_size=3, padding='same')(x)\n",
        "\n",
        "        if (i + 1 + num_layers) % residual_every == 0 and residuals:\n",
        "            res = residuals.pop()\n",
        "            x = Add()([x, res])\n",
        "\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    if residuals: raise ValueError('There are unused residual connections')\n",
        "\n",
        "    # create 1-channel output\n",
        "    x = Conv2DTranspose(1, kernel_size=3, strides=2, padding='same')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def REDNet(num_layers, num_features, channel_size):\n",
        "    '''Model definition with keras functional layers api'''\n",
        "\n",
        "    inputs = Input(shape=(None, None, channel_size))\n",
        "\n",
        "    x, residuals = encoder(inputs, num_features, num_layers)\n",
        "\n",
        "    x = decoder(x, num_features, num_layers, residuals)\n",
        "\n",
        "    # Add input residual, needed to do 1x1 conv to adapt channels\n",
        "    residual = Conv2DTranspose(1, kernel_size=1, padding='same')(inputs)\n",
        "    outputs = Add()([x, residual])\n",
        "    outputs = Activation('relu')(outputs)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs=inputs, outputs=outputs, name=f'REDNet{num_layers*2}')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-1D9y9dvuyA"
      },
      "outputs": [],
      "source": [
        "# compile the model\n",
        "model = REDNet(\n",
        "    num_layers=11,\n",
        "    num_features=64,\n",
        "    channel_size=6\n",
        ")\n",
        "\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.001,\n",
        "    decay_steps=1000,\n",
        "    decay_rate=0.9,\n",
        "    staircase=True\n",
        ")\n",
        "opt = keras.optimizers.Adam(\n",
        "    learning_rate=lr_schedule\n",
        ")\n",
        "loss = keras.losses.MeanSquaredError( reduction=\"sum_over_batch_size\")\n",
        "\n",
        "model.compile(loss=loss,optimizer=opt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3e524oASoJ8"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = Path.cwd() / 'checkpoints_from_scratch'\n",
        "\n",
        "#model.load_weights((Path.cwd() / 'checkpoints' / '04-0.01.keras'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdV3uwPPv2AO",
        "outputId": "9952df75-ff96-4cff-e517-1ceeff76c2ef"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    ModelCheckpoint((checkpoint_dir / 'ep{epoch:02d}_loss{val_loss:.4f}.keras').as_posix(), save_best_only=True),\n",
        "    CSVLogger(checkpoint_dir / 'training_max.csv', append=True),\n",
        "    #keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0005),\n",
        "    #keras.callbacks.ProgbarLogger(count_mode='steps'),\n",
        "    #WandbMetricsLogger(),\n",
        "]\n",
        "# train on the dataset\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=validation_data,\n",
        "    epochs=20,\n",
        "    callbacks=callbacks,\n",
        "    shuffle=True,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDr5Cc3D9yGR",
        "outputId": "a822922a-6ad0-47ec-e678-07c38a454a85"
      },
      "outputs": [],
      "source": [
        "# train on the dataset\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=validation_data,\n",
        "    epochs=10,\n",
        "    callbacks=callbacks,\n",
        "    shuffle=True,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48gRZoDvp3DD",
        "outputId": "ec91fc11-767d-41ee-a969-df8954d1dc59"
      },
      "outputs": [],
      "source": [
        "model.optimizer.learning_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOqM8cFznJR9",
        "outputId": "fd23264b-e594-4424-af52-8f50840f0987"
      },
      "outputs": [],
      "source": [
        "# train on the dataset\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=validation_data,\n",
        "    epochs=40,\n",
        "    callbacks=callbacks,\n",
        "    shuffle=True,\n",
        "    verbose=1,\n",
        "    initial_epoch=31,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCoFj70aqVYt",
        "outputId": "b1a039a1-75a4-442e-b57f-a3896745b724"
      },
      "outputs": [],
      "source": [
        "model.optimizer.learning_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfkhDvqQs7da"
      },
      "outputs": [],
      "source": [
        "opt = keras.optimizers.Adam(\n",
        "    learning_rate=0.0001\n",
        ")\n",
        "loss = keras.losses.MeanSquaredError( reduction=\"sum_over_batch_size\")\n",
        "\n",
        "model.compile(loss=loss,optimizer=opt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhyaEtSutjrz",
        "outputId": "1ff6b003-84cc-40fa-904d-2a411070f8c7"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=validation_data,\n",
        "    epochs=60,\n",
        "    callbacks=callbacks,\n",
        "    shuffle=True,\n",
        "    verbose=1,\n",
        "    initial_epoch=49,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRfBOg4NvgZV"
      },
      "source": [
        "now train on the full size validation, but with a split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIwe0vn-u1eu",
        "outputId": "3c2f1373-097e-4545-cf15-19f66b1cda6f"
      },
      "outputs": [],
      "source": [
        "full_train_data = DataGenerator(\n",
        "    val_set_original_path,\n",
        "    shuffle=False,\n",
        "    random_flip=True,\n",
        "    batch_size=32,\n",
        "    only_use_n=4815-128\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmkpc1UF1YKa",
        "outputId": "1bca492b-0601-4cb5-dcc3-9cc3863464c1"
      },
      "outputs": [],
      "source": [
        "valset = DataGenerator(\n",
        "    val_set_crop_path,\n",
        "    shuffle=False,\n",
        "    random_flip=True,\n",
        ")\n",
        "valset.filenames = valset.filenames[-128:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVlLlTxw0nW_",
        "outputId": "3e10c046-e727-45e8-8326-d263a40252d1"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    full_train_data,\n",
        "    validation_data=valset,\n",
        "    epochs=70,\n",
        "    callbacks=callbacks,\n",
        "    shuffle=True,\n",
        "    verbose=1,\n",
        "    initial_epoch=60,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feoJNOl41UJ3"
      },
      "outputs": [],
      "source": [
        "model.save(checkpoint_dir/'with_retrain.model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiHseHJYAmvB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
