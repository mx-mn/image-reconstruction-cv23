{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import random\n",
        "import keras\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "# import module\n",
        "#import shutil\n",
        "\n",
        "datasets_path = Path().cwd().parent / 'data'\n",
        "#test_set = datasets_path / 'test_set'\n",
        "#val_set = datasets_path / 'validation_set_original'\n",
        "test_set = datasets_path / 'big_test_set'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4wKc59hTLxF"
      },
      "outputs": [],
      "source": [
        "val_set_size = 32\n",
        "map_label_to_name = ['no_person', 'idle','sitting', 'laying']\n",
        "\n",
        "def predict_images(model, dataset, folder: Path):\n",
        "    dataset.batch_size = None\n",
        "    x,y = dataset[0]\n",
        "    preds = model.predict_on_batch(x)\n",
        "\n",
        "    for stack, gt, pred, filename in zip(x,y,preds, dataset.filenames):\n",
        "        sample_num = filename.split('_')[1].split('.')[0]\n",
        "        sample_folder: Path = folder / sample_num\n",
        "        sample_folder.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "        to_pil(pred).save(sample_folder / f'prediction.png')\n",
        "        to_pil(gt.squeeze()).save(sample_folder / f'gt.png')\n",
        "\n",
        "        for i in range(stack.shape[-1]):\n",
        "            plane = stack[:,:,i]\n",
        "            to_pil(plane).save(sample_folder / f'plane{i}.png')\n",
        "\n",
        "def to_pil(img):\n",
        "\n",
        "    assert not np.isnan(img).any(), 'NAN'\n",
        "\n",
        "    grayscale_img = ((img - img.min()) * (1/(img.max() - img.min()) * 255)).astype('uint8')\n",
        "    image = Image.fromarray(grayscale_img.squeeze())\n",
        "    return image\n",
        "\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    def __init__(\n",
        "        self,\n",
        "        basedir: Path,\n",
        "        batch_size: int = None,\n",
        "        included_poses: list = None,\n",
        "        included_trees: list = None,\n",
        "        shuffle=False,\n",
        "        only_use_n: int = None\n",
        "    ):\n",
        "        if not basedir.exists():\n",
        "            ValueError('Datafolder does not exist. Add it to your drive and try again. Maybe restart the runtime.')\n",
        "\n",
        "        self.basedir = basedir\n",
        "        self.batch_size = batch_size\n",
        "        self.included_poses = [map_label_to_name.index(pose) for pose in included_poses] if included_poses is not None else None\n",
        "        self.included_trees  = included_trees\n",
        "        self.filenames = self.__filter(shuffle, only_use_n)\n",
        "\n",
        "    def __filter(self, shuffle, only_use_n):\n",
        "\n",
        "        files = []\n",
        "        self.pose_distribution = defaultdict(int)\n",
        "        self.trees_distribution = defaultdict(int)\n",
        "        self.pose_distribution_filtered = defaultdict(int)\n",
        "        self.trees_distribution_filtered = defaultdict(int)\n",
        "\n",
        "        unfiltered = list(self.basedir.iterdir())\n",
        "\n",
        "        if shuffle:\n",
        "            random.shuffle(unfiltered)\n",
        "\n",
        "        total = len(unfiltered)\n",
        "        if only_use_n is not None:\n",
        "            total = only_use_n\n",
        "\n",
        "        for path in tqdm(unfiltered, total=total):\n",
        "\n",
        "            loaded = np.load(path)\n",
        "            pose, trees = loaded['pose'], loaded['trees']\n",
        "\n",
        "            self.pose_distribution[pose.item()] += 1\n",
        "            self.trees_distribution[trees.item()] += 1\n",
        "\n",
        "            fname = path.name\n",
        "            if self.included_poses is not None and pose not in self.included_poses:\n",
        "                continue\n",
        "\n",
        "            if self.included_trees is not None and trees not in self.included_trees:\n",
        "                continue\n",
        "\n",
        "            files.append(fname)\n",
        "            self.pose_distribution_filtered[pose.item()] += 1\n",
        "            self.trees_distribution_filtered[trees.item()] += 1\n",
        "\n",
        "            if only_use_n is not None and len(files) == only_use_n:\n",
        "                break\n",
        "\n",
        "        return files\n",
        "\n",
        "    def load(self, path):\n",
        "        loaded = np.load(path)\n",
        "        x = loaded['x'] / 255\n",
        "        y = loaded['y'] / 255\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.batch_size is None:\n",
        "            return len(self.filenames)\n",
        "\n",
        "        return math.ceil(len(self.filenames) / self.batch_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if self.batch_size is None:\n",
        "            batch = self.filenames\n",
        "        else:\n",
        "            low = idx * self.batch_size\n",
        "            high = min(low + self.batch_size, len(self.filenames))\n",
        "            batch = self.filenames[low:high]\n",
        "\n",
        "        X, Y = [],[]\n",
        "        for fname in batch:\n",
        "            x,y = self.load(self.basedir / fname)\n",
        "            X.append(x)\n",
        "            Y.append(y)\n",
        "\n",
        "        return np.stack(X), np.stack(Y)\n",
        "\n",
        "    def print_info(self, include_unfiltered=False):\n",
        "        print()\n",
        "        shape = self.load(self.basedir / self.filenames[0])[0].shape\n",
        "        print(f'{len(self.filenames)} samples with shape : {shape}')\n",
        "        if include_unfiltered:\n",
        "            print(f'Pose distribution total')\n",
        "            (\"{:<15} {:<15}\".format('pose', 'number of samples'))\n",
        "            for key, value in self.pose_distribution.items():\n",
        "                print(\"{:<15} {:<15}\".format(map_label_to_name[key], value))\n",
        "        print()\n",
        "        print(f'Pose distribution filtered')\n",
        "        (\"{:<15} {:<15}\".format('pose', 'number of samples'))\n",
        "        for key, value in self.pose_distribution_filtered.items():\n",
        "            print(\"{:<15} {:<15}\".format(map_label_to_name[key], value))\n",
        "\n",
        "        if include_unfiltered:\n",
        "            print()\n",
        "            print(f'Trees distribution total')\n",
        "            print(\"{:<15} {:<15}\".format('num trees per ha', 'number of samples'))\n",
        "\n",
        "            for key, value in self.trees_distribution.items():\n",
        "                print(\"{:<15} {:<15}\".format(key, value))\n",
        "\n",
        "        print()\n",
        "        print(f'Trees distribution filtered')\n",
        "        print(\"{:<15} {:<15}\".format('num trees per ha', 'number of samples'))\n",
        "\n",
        "        for key, value in self.trees_distribution_filtered.items():\n",
        "            print(\"{:<15} {:<15}\".format(key, value))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "datasets = dict()\n",
        "trees = [0,100,200]\n",
        "poses = ['no_person', 'idle','sitting', 'laying']\n",
        "for tree in trees:\n",
        "    for pose in poses:\n",
        "        dataset = DataGenerator(\n",
        "            basedir=test_set, \n",
        "            included_trees=[tree], \n",
        "            included_poses=[pose],\n",
        "            only_use_n=val_set_size\n",
        "        )\n",
        "        print(len(dataset.filenames))\n",
        "        datasets[str(tree)+'_trees_'+pose] =  dataset\n",
        "        #for filename in dataset.filenames:\n",
        "            #shutil.copyfile(val_set / filename, test_set / filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL = 'with_retrain.model.keras'     # trained on full sized val set\n",
        "MODEL = 'MAE_ep108_loss0.0043.keras'   # trained with mean absolute error\n",
        "MODEL = 'ep60_loss0.0051.keras'        # after 60 epochs \n",
        "MODEL = '20240114-195043.model.keras'  # original run\n",
        "model: Model = keras.saving.load_model(Path.cwd().parent/ 'models' / MODEL)\n",
        "\n",
        "predictions_folder = Path.cwd() / 'test_set_pred' / 'oldmodel'\n",
        "\n",
        "for name, dataset in tqdm(datasets.items(), total=len(list(datasets.keys()))):\n",
        "    predict_images(\n",
        "        model, \n",
        "        dataset, \n",
        "        predictions_folder / name\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "combo_path = Path.cwd() / 'test_set_pred' / 'mae_combo_images'\n",
        "combo_path.mkdir(exist_ok=True)\n",
        "for name, dataset in tqdm(datasets.items(), total=len(list(datasets.keys()))):\n",
        "    path: Path = predictions_folder / name\n",
        "    if not path.is_dir(): continue\n",
        "    for sample_path in path.iterdir():\n",
        "        if not sample_path.is_dir(): continue\n",
        "        if '.DS_Store' in sample_path.as_posix(): continue\n",
        "        blank_image = Image.new(\"L\", (512*2 + 256, 512))\n",
        "        gt = Image.open(sample_path / 'gt.png')\n",
        "        pred = Image.open(sample_path / 'prediction.png')\n",
        "        plane1 = Image.open(sample_path / 'plane1.png').resize((256,256))\n",
        "        plane4 = Image.open(sample_path / 'plane4.png').resize((256,256))\n",
        "        blank_image.paste(gt, (0,0))\n",
        "        blank_image.paste(plane1, (512,0))\n",
        "        blank_image.paste(plane4, (512,256))\n",
        "        blank_image.paste(pred, (512+256,0))\n",
        "        #blank_image.save(path / (sample_path.name + 'combo.png'))\n",
        "        blank_image.save(combo_path / (name + '_' + sample_path.name + '.png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# COMPARE different models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_folder_A =  Path.cwd() / 'test_set_pred' / 'extra_mae'\n",
        "predictions_folder_B =  Path.cwd() / 'test_set_pred' / 'oldmodel'\n",
        "compare_path = Path.cwd() / 'test_set_pred' / 'a_mae_b_oldmodel'\n",
        "compare_path.mkdir(exist_ok=True, )\n",
        "\n",
        "for name, dataset in tqdm(datasets.items(), total=len(list(datasets.keys()))):\n",
        "\n",
        "    samples_dir_A = predictions_folder_A / name\n",
        "    samples_dir_B = predictions_folder_B / name\n",
        "    if not samples_dir_A.is_dir(): continue\n",
        "    if not samples_dir_B.is_dir(): continue\n",
        "\n",
        "    # loop over all samples\n",
        "    for sample_path in samples_dir_A.iterdir():\n",
        "        if not sample_path.is_dir(): continue\n",
        "        if '.DS_Store' in sample_path.as_posix(): continue\n",
        "\n",
        "        path_a = samples_dir_A / sample_path.name\n",
        "        path_b = samples_dir_B / sample_path.name\n",
        "\n",
        "\n",
        "        blank_image = Image.new(\"L\", (512*3, 512))\n",
        "        gt = Image.open(sample_path / 'gt.png')\n",
        "        pred_a = Image.open(path_a / 'prediction.png')\n",
        "        pred_b = Image.open(path_b / 'prediction.png')\n",
        "        #plane1 = Image.open(sample_path / 'plane1.png').resize((256,256))\n",
        "        #plane4 = Image.open(sample_path / 'plane4.png').resize((256,256))\n",
        "        \n",
        "        blank_image.paste(gt, (512,0))\n",
        "        blank_image.paste(pred_a, (0,0))\n",
        "        blank_image.paste(pred_b, (1024,0))\n",
        "        #blank_image.paste(plane1, (512,0))\n",
        "        #blank_image.paste(plane4, (512,256))\n",
        "        #blank_image.save(path / (sample_path.name + 'combo.png'))\n",
        "        blank_image.save(compare_path / (name + '_' + sample_path.name + '.png'))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
